[["index.html", "Notes for Math, Probability, and Statistics Chapter 1 Contents", " Notes for Math, Probability, and Statistics Jessica B 2023-05-26 Chapter 1 Contents "],["axioms-of-kolmogorov.html", "Chapter 2 Axioms of Kolmogorov 2.1 Sets, Sample Spaces, and events 2.2 Axioms of Kolmogorov 2.3 Implied consequences of the Axioms 2.4 CONTINUITY OF PROBABILITY FUNCTIONS", " Chapter 2 Axioms of Kolmogorov There are three rules defined for creating a probabilistic model: The probability of an event must be greater than zero That the probabilities for all disjoint events in a sample space must sum to one The total probability for the union of events is equal to the sum of their individual probabilities. This is an ultra simplified statement of the axioms that govern modern probability theory. In 1933, these rules were formally axiomized by the great Andrey Nikolaevich Kolmogorov. Before describing the axioms more formally, lets review some set theory. 2.1 Sets, Sample Spaces, and events Sets are collections of distinct elements. A sample space is the set of disjoint, collectively exhaustive outcomes taken at a determined level of granularity. A sample space can be described as finite or infinite, discrete or continuous. 2.1.1 Definitions and LaTex code. Symbol Name LaTeX \\(\\sim\\) Similar \\sim \\(=\\) Equality, sets are identical i.e. contain the same elements = \\(\\cup\\) Intersection \\cup \\(\\cap\\) Union \\cap \\(\\in\\) Belongs to \\in \\(\\varnothing\\) Null set \\varnothing \\(\\Omega\\), \\(S\\) Universal set \\Omega \\(\\omega\\), \\(s\\) Atom, singleton \\omega \\(\\subset\\) Proper subset, S is contained within A, sometimes used S = A \\subset \\(\\subseteq\\) Subset, if S is contained within A \\subseteq \\(\\bigcup\\) infinitary union, generalized union, unified union \\bigcup \\(\\bigcap\\) infinitary intersection, generalized intersection, unified intersection cap \\bigcap 2.1.2 More Set Definitions complement, sum, difference, certainty, impossibility, mutual exclusivity.. 2.1.3 Relational Laws Communicative Associative Distributive DeMorgan’s 1st and 2nd laws [add proofs, also identities are usually proven elementwise] 2.2 Axioms of Kolmogorov Non-negativity Axiom \\[ \\text{For any event A in our sample space, } \\mathbb{P}(A)≥0 \\] Unity Axiom \\[ \\text{Normalization, also called the assumption of unit measure,}\\\\ \\text{defines the probability of the sample space } \\Omega \\text{ as } \\mathbb{P}(\\Omega)=1 \\] \\(\\sigma\\)-additivity Axiom Any countable pairwise disjoint sequence (i.e. A1,A2,...), even countably infinite sequences of events satisfies \\(P(\\bigcup_{i=1}^{\\infty}A_{i}) = \\sum_{n=1}^{\\infty}P(A_{i})\\) where pairwise disjoint: i.e. \\(A_iA_j = \\varnothing\\), where \\(i \\neq j\\) (countable sequence of events: \\(A_1,A_2,...\\)) Finite additivity is 2.3 Implied consequences of the Axioms Demonstrating these immediate consequences illustrates the power of the third axiom, and its interaction with the remaining two axioms. (Theorem 1.1) Probability of the empty set \\(P(\\varnothing) = 0\\) (Theorem 1.2) Finite Additivity (Theorem 1.3) “Average” (Theorem 1.4) The complement rule (Theorem 1.5) Monotonicity Corollary: The numeric bound (Theorem 1.6) Inclusion-Exclusion Principal (Theorem 1.7) P(A) = P(AB) + P(AB^C) (Theorem 1.8) Continuity of Probability Function Booles ineqauilty 2.3.1 A word of Probability Measures 2.4 CONTINUITY OF PROBABILITY FUNCTIONS "],["combinatorics.html", "Chapter 3 Combinatorics 3.1 Counting Principal 3.2 Number of Sets in a Subset 3.3 Permutations 3.4 Combinations", " Chapter 3 Combinatorics 3.1 Counting Principal Fundamental Counting Principle a.k.a. generalized counting principal, Also known as the rule of product, or the multiplication principal. Let \\(E_1, E_2, ..., E_k\\) be sets with \\(n_1,n_2, ...,n_k\\) elements,respectively. Then there are \\(n_1×n_2×n_3×···×n_k\\) ways in which we can, first, choose an element of \\(E_1\\), then an element of \\(E_2\\), then an element of \\(E_3\\)… and finally an element of \\(E_k\\). Combinatorial principles - Wikipedia 3.2 Number of Sets in a Subset The set of all subsets of A is called the power set of A. We can calculate the number of subsets in a set as \\(2^n\\), because each element can be either included or excluded (hence the two options) Mathematically we can say that 3.3 Permutations A permutation is the number of ways we can order \\(n\\) distinct objects, from the general counting principal it follows that we have \\(n\\) ways to make the first selection, the we will have one less ways to make our next selection, and so on. For example if we have ten objects, there is ten ways to… \\[ (n)(n−1)(n−2)···(1)=n! \\] This is because of there are of the ways to \\(n\\) ways to choose the first object, n-1 ways to choose the second, all the way to only \\(1\\) possible way to choose the last object. We use the notation \\(_nP_r\\) to denote the number of permutations of a set \\(A\\) containing \\(n\\) elements taken \\(r\\) at a time where \\((1 ≤ r ≤ n)\\) The number of r-element permutations of a set containing n objects is given by \\[ _nP_r = \\frac{n!}{(n-r)!} \\] (note: to avoid division by zero when \\(n=r\\) we define \\(0! = 1\\)) To derive the formula for \\(_nP_r\\) we consider the number of choices for each selection. Since \\(A\\) has \\(n\\) elements, the number of choices for the first object in the \\(r\\)-element permutation is \\(n\\). For the next selection there will be \\(n-1\\) options, followed by \\(n-2\\) options for the selection that follows after that, we do this until our last selection, which is the \\(r\\)th object. This object has possible \\(n − (r − 1)\\) choices. This leads us to the equation \\[ _nP_r =n(n−1)(n−2)···(n−r+1) \\] We can also note how this relates to a simple permutation which we discussed above \\[ _nP_n =n(n−1)(n−2)···(n−n+1)=n! \\] How we get our general formula \\(_nP_r\\) is by manipulating the equation \\(n(n−1)(n−2)···(n−r+1)\\) which is done by multiplying both sides by \\((n-r)!\\), then dividing both sides by \\((n-r)!\\) Which leads us to the formula \\[ _nP_r = \\frac{n!}{(n-r)!} \\] [reorganize to follow and add actual derivation] However the formula above is valid only if all of the objects of the set are distinguishable from each other. If there are repeated elements (e.g. the letter E in the word “BERKELEY”) we must take add [karapanian veusus brungard] \\[ \\frac{n!}{n_1! × n_2 ! × · · · × n_k!} \\] An example of repeated elements in a permutation problem would be: How many different 10-letter codes can be made using three a’s, four b’s, and three c’s? Which would have the solution \\(\\frac{10!}{(3!*4!*3!)}\\), since you need to use to above theorem to divide out the elements that would otherwise be over-counted. 3.4 Combinations [motivation add how its different than combination] we have for \\(r \\le n\\) \\[ \\binom{n}{r}=\\frac{n!}{r!(n-r)!} \\] We are calculating the number of \\(r\\)-element subsets from a \\(n\\)-element set, where order matters. This is the number of all r-element combinations of n objects, where we only use valid non-negative integers (e.g. 0,1,2, …) where \\(r \\le n\\). [add derivation!!!!! xr! = npr] consider \\[ \\binom{n}{n}=\\binom{n}{0}=1 \\] \\[ \\binom{n}{n}=\\frac{n!}{n!(n-n)!=1} \\] consider \\[ \\binom{n}{1}=\\binom{n}{n-1} \\] consider where \\(0 \\le r \\le n\\) \\[ \\binom{n}{r}=\\binom{n}{n-r} \\] and \\[ \\binom{n+1}{r}=\\binom{n}{r}+\\binom{n}{r-1} \\] [write more thoughts about this and how to verify algebraically or using combinatorics] [add unordered sampling with replacement] [example 2.22 and binomial expansion] "],["conditional-probability.html", "Chapter 4 Conditional Probability 4.1 Conditional Probability is a Probability Measure 4.2 Reduction of a Sample Space 4.3 Law of Multiplication 4.4 Law of Total Probability 4.5 Bayes’ Formula", " Chapter 4 Conditional Probability Conditional probability is not a theorem, nor a conjecture. It is not a lemma or a corollary. Instead conditional probability is a fundamental concept which is inherently understood…. It is obvious that the conditional probability \\(\\mathbb{P}(A|B)\\) gives an advantage when knowing the occurrence of B changes the occurrence of A. The probability of \\(\\mathbb{P}(A|B)\\) is obviously given by the ratio of the relative frequency of the intersection of A and B and the relative frequency of B. We can also state this as “the ratio of the probability of joint occurrence of A and B and the probability of B.” Conditional probability can be can be visualized when we think in terms of area…. We define conditional probability as \\[ \\mathbb{P}(A|B)=\\frac{\\mathbb{P}(AB)}{\\mathbb{P}(B)} \\] where \\(\\mathbb{P}(B)&gt;0\\). [Supporting example coming] Intersection versus joint occurance of events… I thought it was the same thing. But I found myself getting bogged down in set theory. We know of course that P(AB) is the intersection of common elements between sets A and B. The joint occurance better describles the probability that events are to occur. 4.1 Conditional Probability is a Probability Measure That is, they satisfy the same axioms that ordinary probabilities satisfy. This enables us to use the theorems that are true for probabilities for conditional probabilities as well. [Proof coming soon] 4.2 Reduction of a Sample Space It is possible to reduce a sample space e.g. from (S to B) and have a smaller set of subsets. Making it easier to calculate conditioned probabilities. 4.3 Law of Multiplication Understand that conditional probability represents the relation between \\(\\mathbb{P}(A|B)\\) and \\(\\frac{\\mathbb{P}(AB)}{\\mathbb{P}(B)}\\) where \\(\\mathbb{P}(B) &gt; 0\\). By simply multiplying both sides by the \\(\\mathbb{P}(B)\\), The law of multiplication demonstrates that probability of the joint occurrence of A and B is the product of the probability of B and the conditional probability of A given that B has occurred. \\[ \\mathbb{P}(AB)=\\mathbb{P}(A|B)\\mathbb{P}(B), \\text{ where } \\mathbb{P}(B)&gt;0 \\] also, \\[ \\mathbb{P}(AB)=\\mathbb{P}(BA)=\\mathbb{P}(B|A)\\mathbb{P}(A), \\text{ where } \\mathbb{P}(A) &gt; 0 \\] We can also calculate the joint probability of three events by using association laws, e.g. \\(P(ABC)=P(A(BC))=\\mathbb{P}(A)P(B|A)P()\\) Generalizing these theorems stated can be extended to \\(n\\) events to \\(k\\) events, the resulting formula can be proved by mathematical induction. The theorem is as follows if \\(P(A_1 A_2 A_3 ...A_{n−1}) &gt; 0\\), then \\[ P(A_1 A_2 A_3 ...A_{n−1}A_n) = P(A_1)P(A_2 | A_1)P(A_3 | A_1A_2)···P(A_n | A_1A_2A_3 ···A_{n−1}) \\] 4.4 Law of Total Probability \\[ \\mathbb{P}(A) = \\mathbb{P}(A|B)\\mathbb{P}(A) + \\mathbb{P}(A|B^c)\\mathbb{P}(B^c) \\] Proof: Theorem 1.7 To generalize the theorem we define the partion of \\(S\\) 4.5 Bayes’ Formula Bayes’ Rule is a simple calculation but a big idea about beliefs. Consider that the partitioned sample space we imagine for the law of total probability. With Bayes’ Rule we are update our belief about something given that we know some other event is true or has occurred. We want to calculated the probability of an unknown event occur Think of it has Hypothesis given evidence….. More literally it is hypothesis given evidence is true \\[ P(H|E) \\] Prior: In order to update our beliefs… we need to consider our prior knowledge \\(P(H)\\) Liklihood: Our subset where we consider the liklihood that our hypothesis is true under the conditional evidence \\(P(E|H)\\) Post "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
